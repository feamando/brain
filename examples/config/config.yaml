# PM-OS Brain Configuration
# Copy this file to your user directory and customize

# User identification
user:
  name: "Your Name"
  email: "your.email@example.com"
  slack_id: "U0123456789"

# Brain settings
brain:
  # Path to brain data directory (relative to user root)
  path: "brain"
  
  # Entity types to index
  entity_types:
    - person
    - team
    - project
    - squad
    - event
  
  # Auto-enrichment settings
  enrichment:
    enabled: true
    sources:
      - slack
      - jira
      - github
    # How often to run enrichment (hours)
    interval: 24
  
  # Search settings
  search:
    # Maximum results to return
    max_results: 50
    # Fuzzy matching threshold (0-100)
    fuzzy_threshold: 80

# Integration credentials (or use .env file)
integrations:
  slack:
    enabled: true
    # bot_token: "xoxb-..." # Better to use SLACK_BOT_TOKEN env var
  
  jira:
    enabled: true
    url: "https://your-company.atlassian.net"
    # username and api_token in .env
  
  github:
    enabled: true
    org: "your-org"
    # token in .env

# =============================================================================
# LLM Configuration
# =============================================================================
# Brain uses LLMs for:
#   - Entity extraction from unstructured text
#   - Relationship inference
#   - Semantic search via embeddings
#   - Content summarization
#
# Configure your preferred provider(s) below. API keys should go in .env file.

llm:
  # Primary provider to use
  # Options: openai, anthropic, gemini, mistral, ollama, azure, bedrock
  provider: anthropic
  
  # Fallback providers if primary fails (in order)
  fallback:
    - openai
    - gemini
    - ollama
  
  # Provider-specific settings
  providers:
    # --- OpenAI ---
    openai:
      model: gpt-4o                          # Best quality
      # model: gpt-4-turbo                   # Fast + capable
      # model: gpt-3.5-turbo                 # Budget option
      embedding_model: text-embedding-3-small
      max_tokens: 4096
      temperature: 0.1                       # Low for factual extraction
    
    # --- Anthropic (Claude) ---
    anthropic:
      model: claude-sonnet-4-20250514               # Balanced performance
      # model: claude-opus-4-20250514                # Highest quality
      # model: claude-3-haiku-20240307       # Fast + cheap
      max_tokens: 4096
      temperature: 0.1
    
    # --- Google Gemini ---
    gemini:
      model: gemini-2.0-flash                # Fast + capable
      # model: gemini-1.5-pro                # Higher quality
      # model: gemini-1.5-flash              # Budget option
      max_tokens: 4096
      temperature: 0.1
    
    # --- Mistral ---
    mistral:
      model: mistral-large-latest            # Best quality
      # model: mistral-medium-latest         # Balanced
      # model: mistral-small-latest          # Fast + cheap
      # model: open-mistral-7b               # Open source
      embedding_model: mistral-embed
      max_tokens: 4096
      temperature: 0.1
    
    # --- Ollama (Local/Self-hosted) ---
    # Great for privacy-sensitive deployments or offline use
    ollama:
      base_url: http://localhost:11434
      model: llama3                          # Meta's Llama 3
      # model: mistral                       # Mistral 7B
      # model: codellama                     # Code-focused
      # model: phi3                          # Microsoft Phi-3
      # model: gemma2                        # Google Gemma 2
      max_tokens: 4096
      temperature: 0.1
    
    # --- Azure OpenAI (Enterprise) ---
    azure:
      endpoint: https://your-resource.openai.azure.com
      deployment: your-deployment-name
      api_version: "2024-02-01"
      max_tokens: 4096
      temperature: 0.1
    
    # --- AWS Bedrock ---
    bedrock:
      region: us-east-1
      model_id: anthropic.claude-3-sonnet-20240229-v1:0
      # model_id: anthropic.claude-3-haiku-20240307-v1:0
      # model_id: amazon.titan-text-express-v1
      # model_id: meta.llama3-70b-instruct-v1:0
      max_tokens: 4096
      temperature: 0.1

  # Task-specific model overrides
  # Use different models for different tasks based on cost/quality tradeoffs
  tasks:
    # Entity extraction needs good comprehension
    entity_extraction:
      provider: anthropic
      model: claude-sonnet-4-20250514
    
    # Embeddings can use cheaper/faster models
    embeddings:
      provider: openai
      model: text-embedding-3-small
      # Or use Mistral embeddings:
      # provider: mistral
      # model: mistral-embed
    
    # Summarization can use fast models
    summarization:
      provider: gemini
      model: gemini-2.0-flash
    
    # Relationship inference needs reasoning
    relationship_inference:
      provider: anthropic
      model: claude-sonnet-4-20250514

# Quality settings
quality:
  # Minimum completeness score for entities (0-100)
  min_completeness: 60
  # Warn on stale entities older than (days)
  stale_threshold: 90
  # Required fields for each entity type
  required_fields:
    person:
      - name
      - role
    project:
      - name
      - status
      - owner
    team:
      - name
      - members

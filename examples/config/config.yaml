# PM-OS Brain Configuration
# Copy this file to your user directory and customize

user:
  name: "Your Name"
  email: "your.email@example.com"
  slack_id: "U0123456789"

brain:
  path: "brain"
  entity_types: [person, team, project, squad, event]
  enrichment:
    enabled: true
    sources: [slack, jira, github]
    interval: 24
  search:
    max_results: 50
    fuzzy_threshold: 80

integrations:
  slack:
    enabled: true
  jira:
    enabled: true
    url: "https://your-company.atlassian.net"
  github:
    enabled: true
    org: "your-org"

# =============================================================================
# LLM Configuration - Updated February 2026
# =============================================================================
llm:
  provider: anthropic
  fallback: [openai, gemini, ollama]
  
  providers:
    # --- OpenAI (Latest: January 2026) ---
    openai:
      model: gpt-4o                          # Latest multimodal flagship
      # model: o1                            # Reasoning model (best for complex tasks)
      # model: o3-mini                       # Fast reasoning model
      # model: gpt-4o-mini                   # Budget option
      embedding_model: text-embedding-3-large  # Best quality embeddings
      # embedding_model: text-embedding-3-small  # Budget embeddings
      max_tokens: 16384
      temperature: 0.1
    
    # --- Anthropic Claude (Latest: February 2026) ---
    anthropic:
      model: claude-sonnet-4-20250514               # Latest balanced model (recommended)
      # model: claude-opus-4-20250514                # Highest capability
      # model: claude-3-5-haiku-20241022    # Fast & efficient
      max_tokens: 8192
      temperature: 0.1
    
    # --- Google Gemini (Latest: January 2026) ---
    gemini:
      model: gemini-2.0-flash-exp           # Latest experimental flash
      # model: gemini-2.0-pro-exp           # Latest experimental pro
      # model: gemini-1.5-pro-002           # Stable pro
      # model: gemini-1.5-flash-002         # Stable flash
      max_tokens: 8192
      temperature: 0.1
    
    # --- Mistral (Latest: January 2026) ---
    mistral:
      model: mistral-large-2411             # Latest large (Nov 2024)
      # model: mistral-small-2409           # Latest small (Sep 2024)
      # model: codestral-2405               # Code-specialized
      # model: ministral-8b-2410            # Edge/efficient model
      # model: pixtral-large-2411           # Vision model
      embedding_model: mistral-embed
      max_tokens: 8192
      temperature: 0.1
    
    # --- Ollama (Local - Latest Models) ---
    ollama:
      base_url: http://localhost:11434
      model: llama3.2                        # Meta Llama 3.2
      # model: llama3.3:70b                 # Llama 3.3 70B
      # model: qwen2.5:72b                  # Alibaba Qwen 2.5
      # model: deepseek-r1:14b              # DeepSeek R1 reasoning
      # model: phi4                         # Microsoft Phi-4
      # model: gemma2:27b                   # Google Gemma 2
      # model: mistral-nemo                 # Mistral Nemo 12B
      max_tokens: 4096
      temperature: 0.1
    
    # --- Azure OpenAI ---
    azure:
      endpoint: https://your-resource.openai.azure.com
      deployment: your-deployment-name
      api_version: "2024-10-21"
      max_tokens: 16384
      temperature: 0.1
    
    # --- AWS Bedrock (Latest) ---
    bedrock:
      region: us-east-1
      model_id: anthropic.claude-3-5-sonnet-20241022-v2:0
      # model_id: anthropic.claude-3-5-haiku-20241022-v1:0
      # model_id: amazon.nova-pro-v1:0      # Amazon Nova Pro
      # model_id: amazon.nova-lite-v1:0     # Amazon Nova Lite
      # model_id: meta.llama3-2-90b-instruct-v1:0
      max_tokens: 8192
      temperature: 0.1
    
    # --- Groq (Ultra-fast inference) ---
    groq:
      model: llama-3.3-70b-versatile        # Llama 3.3 70B
      # model: llama-3.2-90b-vision-preview # Vision model
      # model: mixtral-8x7b-32768           # Mixtral
      max_tokens: 8192
      temperature: 0.1
    
    # --- Together AI (Open models) ---
    together:
      model: meta-llama/Llama-3.3-70B-Instruct-Turbo
      # model: Qwen/Qwen2.5-72B-Instruct-Turbo
      # model: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
      max_tokens: 8192
      temperature: 0.1

  # Task-specific model overrides
  tasks:
    entity_extraction:
      provider: anthropic
      model: claude-sonnet-4-20250514
    embeddings:
      provider: openai
      model: text-embedding-3-large
    summarization:
      provider: gemini
      model: gemini-2.0-flash-exp
    relationship_inference:
      provider: anthropic
      model: claude-sonnet-4-20250514
    code_analysis:
      provider: anthropic
      model: claude-sonnet-4-20250514       # or use codestral for Mistral

quality:
  min_completeness: 60
  stale_threshold: 90
  required_fields:
    person: [name, role]
    project: [name, status, owner]
    team: [name, members]
